# Copy this file to .env.local and fill in your values

# =============================================================================
# Databricks Configuration
# =============================================================================
# Workspace URL and personal access token for local development
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=dapi...

# =============================================================================
# Database Configuration (Lakebase)
# =============================================================================
# Choose ONE of the following options:

# Option 1: Dynamic OAuth (recommended for Databricks Apps deployment)
# Uses Databricks SDK to generate OAuth tokens automatically
LAKEBASE_INSTANCE_NAME=your-lakebase-instance
LAKEBASE_DATABASE_NAME=databricks_postgres

# Option 2: Static connection URL (for local development)
# LAKEBASE_PG_URL=postgresql://user:password@host:5432/database?sslmode=require

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose your LLM provider: DATABRICKS (default) or AZURE
LLM_PROVIDER=DATABRICKS

# Databricks Foundation Models (adjust to models available in your workspace)
# Examples: databricks-meta-llama-3-3-70b-instruct, databricks-claude-sonnet-4
DATABRICKS_MODEL=databricks-meta-llama-3-3-70b-instruct
DATABRICKS_MODEL_MINI=databricks-gemini-3-flash

# Azure OpenAI (uncomment if using Azure instead)
# LLM_PROVIDER=AZURE
# AZURE_OPENAI_API_KEY=your-api-key
# AZURE_OPENAI_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
# AZURE_OPENAI_DEPLOYMENT=gpt-4o
# AZURE_OPENAI_DEPLOYMENT_MINI=gpt-4o-mini

# =============================================================================
# Skills Configuration
# =============================================================================
# Skills to include (comma-separated list of skill folder names)
ENABLED_SKILLS=databricks-python-sdk,spark-declarative-pipelines,synthetic-data-generation

# Optional: Add additional skills
# ENABLED_SKILLS=databricks-python-sdk,spark-declarative-pipelines,synthetic-data-generation,unstructured-pdf-generation,agent-bricks

# Test mode: only enable Skill tool (useful for debugging)
SKILLS_ONLY_MODE=false

# =============================================================================
# Application Settings
# =============================================================================
# Projects directory (where Claude Code agent will work)
PROJECTS_BASE_DIR=./projects

# Environment (development or production)
ENV=development

# Claude SDK stream timeout in milliseconds (default: 1 hour)
# Increase if you have very long-running operations
CLAUDE_CODE_STREAM_CLOSE_TIMEOUT=3600000

# Anthropic API key (optional - uses Databricks model serving by default)
# ANTHROPIC_API_KEY=sk-ant-...
